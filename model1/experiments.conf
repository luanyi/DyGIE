# Word embeddings.


glove_300d_filtered {
  path = ../embeddings/glove.840B.300d.txt.filtered
  size = 300
  format = txt
  lowercase = false
}


glove_300d_2w {
  path = ../embeddings/glove_50_300_2.txt
  size = 300
  format = txt
  lowercase = false
}

glove_300d_filtered_genia{
path = ../embeddings/glove.840B.300d.txt.filtered.genia
size = 300
format = txt
lowercase = false
}
glove_300d_filtered_wlp{
path = ../embeddings/glove.840B.300d.txt.filtered.wlp
size = 300
format = txt
lowercase = false
}
# Main configuration.
best {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "../embeddings/char_vocab_old.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualizer = lstm
  contextualization_size = 200
  contextualization_layers = 3
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  use_metadata = true
  use_features = true
  model_heads = true
  lm_layers = 3
  lm_size = 1024
  sva = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  const_weight = 0  # 0.1
  ner_weight = 0  # 0.1

  report_frequency = 250
  log_root = logs
}

mtl_best = ${best} {
  char_embedding_size = 8
  contextualization_layers = 3
  contextualization_size = 200

  use_features = true
  use_metadata = true
  model_heads = true
  task_heads = false

  max_arg_width = 30
  argument_ratio = 0.8
  predicate_ratio = 0.4
  mention_ratio = 0.4

  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100

  num_attention_heads = 1
  span_score_weight = 0.0

  coref_loss = "mention_rank"
  enforce_srl_constraint = false
  filter_v_args = true
  use_gold_predicates = false
  srl_weight = 0
  ner_weight = 1.0
  coref_weight = 1.0
  const_weight = 0.0

  batch_size = 40
  max_tokens_per_batch = 700

  # Updated dataset.
  train_path = train.english.mtl.jsonlines
  eval_path = dev.english.mtl.jsonlines
  lm_layers = 3
  lm_size = 1024
  main_metrics = srl_coref_ner

}


DyGIE_best = ${mtl_best}{
  include_c_v = false
  contextualization_layers = 1
  ner_conll_eval_path = ""
  report_frequency = 25
  eval_sleep_secs = 10
  filter_reverse_relations = true
  max_arg_width = 10
  use_metadata = false

  main_metrics = coref_ner_relations
  mention_ratio = 0.3
  max_arg_width = 8
  add_ner_emb = 0
  rel_prop_emb = 0
  
  dropout_rate = 0.4
  ner_weight = 0
  learning_rate = 0.001
  main_metrics = relations

  ner_labels = ["LOC", "WEA", "GPE", "PER", "FAC", "ORG", "VEH"]
  relation_labels = ["PHYS", "PART-WHOLE", "ART", "ORG-AFF", "PER-SOC", "GEN-AFF"]
  ns_randp = 1
  gpu = 0
  relation_weight = 1.0
  coref_only = 0
  rel_prop = 0

  bilinear = 0
  ner_weight = 1
  batch_size = 30
  gpu=2
  coref_depth = 1
  coarse_to_fine = 1
  train_path_coref = "./data/conll/json/train.english.json"
  lm_path_coref = "./data/conll/elmo/train.english.hdf5"
  eval_path_coref = "./data/conll/json/dev.english.json"
  lm_path_dev_coref = "./data/conll/elmo/dev.english.hdf5"
  coref_weight = 0
  entity_ratio = 0.5
  entity_beam = 0
  coref_freq = 2
}
#############ENTITY AND RELATION EXTRACTION #####################
#Best ACE2005 parameters for relation extraction
ace05_best_relation = ${DyGIE_best}{
  lm_path = "../data/ace05/elmo/train.hdf5"
  lm_path_dev = "../data/ace05/elmo/dev.hdf5"
  train_path = "../data/ace05/json/train.json"
  eval_path = "../data/ace05/json/dev.json"
  rel_prop = 3
  gpu = 2
  coref_freq = 0
  coref_depth = 0
}

#Best Wet Lab Protocal (WLP) parameters for relation extraction
wlp_best_relation = ${ace05_best_relation}{
  ner_labels = ["Action", "Reagent", "Location", "Amount", "Modifier", "Time","Device", "Temperature", "Concentration", "Method", "Speed", "Numerical", "Generic-Measure", "Size", "Measure-Type", "Seal", "Mention", "pH"]
  relation_labels = ["Mod-Link", "Meronym", "Creates", "Acts-on", "Measure-Type-Link", "Using", "Site", "Setting", "Count", "Or", "Of-Type", "Coreference", "Measure"]
  lm_path = "../data/wlp/elmo/train.hdf5"
  train_path = "../data/wlp/json/train_3.json"
  lm_path_dev = "../data/wlp/elmo/dev.hdf5"
  eval_path = "../data/wlp/json/dev_3.json"
  main_metrics = relations
  gpu = 1
  relation_weight = 1
  context_embeddings = ${glove_300d_filtered_wlp}
  batch_size = 10
  mention_ratio = 0.3
  entity_ratio = 0.5
  learning_rate = 0.005
  max_arg_width = 8
}

#Best Wet Lab Protocal (WLP) parameters for entity extraction
wlp_best_ner = ${wlp_best_relation}{
  gpu = 2
  main_metrics = ner
  ner_weight = 1
  relation_weight = 0
  lm_path_dev = "../data/wlp/elmo/dev.hdf5"
  eval_path = "../data/wlp/json/dev_3.json"
}

#Best SciERC parameters for entity extraction
scierc_best_ner = ${DyGIE_best}{
  ner_labels = ["Task", "Generic", "Metric", "Material", "OtherScientificTerm", "Method"]
  relation_labels = ["COMPARE", "PART-OF", "FEATURE-OF", "USED-FOR", "CONJUNCTION", "HYPONYM-OF", "EVALUATE-FOR"]
  main_metrics = ner
  relation_weight = 1
  ner_weight = 1
  gpu = 0
  lm_path = "../data/sciERC/elmo/train.hdf5"
  lm_path_dev = "../data/sciERC/elmo/dev.hdf5"
  train_path = "../data/sciERC/json/train.json"
  eval_path = "../data/sciERC/json/dev.json"
  lm_path_coref = "../data/sciERC/elmo/train.hdf5"
  lm_path_dev_coref = "../data/sciERC/elmo/dev.hdf5"
  train_path_coref = "../data/sciERC/json/train.json"
  eval_path_coref = "../data/sciERC/json/dev.json"
  rel_prop = 3
  coref_depth = 2
}

#Best SciERC parameters for relation extraction
scierc_best_relation = ${scierc_best_ner}{
  rel_prop_emb = 1
  rel_prop = 2
  coref_depth = 1
  main_metrics = relations
}


##############OVERLAPPING ENTITY DETECTION##############

#ACE2004-O parameters
ace2004_best_nested_ner = ${DyGIE_best}{
  lm_path = "../data/ace_2004_nested/elmo/train_2004.hdf5"
  train_path = "../data/ace_2004_nested/json/train_2004.json"
  eval_path = "../data/ace_2004_nested/json/dev_2004.json"
  lm_path_dev = "../data/ace_2004_nested/elmo/dev_2004.hdf5"
  gpu = 2
  relation_weight = 0
  main_metrics = ner
}

#ACE2005-O parameters
ace2005_best_nested_ner = ${DyGIE_best}{
  lm_path = "../data/ace_2005_nested/elmo/train_2005.hdf5"
  train_path = "../data/ace_2005_nested/json/train_2005.json"
  eval_path = "../data/ace_2005_nested/json/dev_2005.json"
  lm_path_dev = "../data/ace_2005_nested/elmo/dev_2005.hdf5"
  gpu = 0
  relation_weight = 0
  main_metrics = ner
}

#GENIA parameters
genia_bast_ner = ${DyGIE_best}{
  coref_depth = 2
  lm_path = "../data/genia/elmo/train.hdf5"
  train_path = "../data/genia/json-coref-ident-only/train.json"
  lm_path_dev = "../data/genia/elmo/dev.hdf5"
  eval_path = "../data/genia/json-coref-ident-only/dev.json"
  main_metrics = ner
  ner_labels = ["DNA", "RNA", "cell_line", "cell_type", "protein"]
  relation_weight = 0
  train_path_coref = "../data/genia/json-coref-ident-only/train.json"
  lm_path_coref = "../data/genia/elmo/train.hdf5"
  eval_path_coref = "../data/genia/json-coref-ident-only/dev.json"
  lm_path_dev_coref = "../data/genia/elmo/dev.hdf5"
  batch_size = 20
  gpu = 2
  relation_weight = 0
  context_embeddings = ${glove_300d_filtered_genia}
  learning_rate = 0.0005
}



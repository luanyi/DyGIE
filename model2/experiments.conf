# Word embeddings.


glove_300d_filtered {
  path = ../embeddings/glove.840B.300d.txt.filtered
  size = 300
  format = txt
  lowercase = false
}


glove_300d_2w {
  path = ../embeddings/glove_50_300_2.txt
  size = 300
  format = txt
  lowercase = false
}


# Main configuration.
best {
  # Computation limits.
  max_antecedents = 250
  max_training_sentences = 50
  top_span_ratio = 0.4

  # Model hyperparameters.
  filter_widths = [3, 4, 5]
  filter_size = 50
  char_embedding_size = 8
  char_vocab_path = "../embeddings/char_vocab_old.english.txt"
  context_embeddings = ${glove_300d_filtered}
  head_embeddings = ${glove_300d_2w}
  contextualizer = lstm
  ffnn_size = 150
  ffnn_depth = 2
  feature_size = 20
  max_span_width = 30
  use_metadata = true
  use_features = true
v  model_heads = true
  lm_layers = 3
  lm_size = 1024
  sva = false

  # Learning hyperparameters.
  max_gradient_norm = 5.0
  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  const_weight = 0  # 0.1
  ner_weight = 0  # 0.1

  log_root = logs
}

mtl_best = ${best} {
  char_embedding_size = 8
  contextualization_size = 200

  use_features = true
  use_metadata = true
  model_heads = true
  task_heads = false

  max_arg_width = 30
  argument_ratio = 0.8
  predicate_ratio = 0.4
  mention_ratio = 0.4

  lexical_dropout_rate = 0.5
  dropout_rate = 0.2
  lstm_dropout_rate = 0.4
  optimizer = adam
  learning_rate = 0.001
  decay_rate = 0.999
  decay_frequency = 100
  num_attention_heads = 1
  span_score_weight = 0.0


  coref_loss = "mention_rank"
  enforce_srl_constraint = false
  filter_v_args = true
  use_gold_predicates = false
  srl_weight = 0
  ner_weight = 1.0
  const_weight = 0.0

  max_tokens_per_batch = 700

  # Updated dataset.
  train_path = train.english.mtl.jsonlines
  eval_path = dev.english.mtl.jsonlines
  lm_layers = 3
  lm_size = 1024
  main_metrics = srl_coref_ner

  include_c_v = false
  contextualization_layers = 1

  ner_conll_eval_path = ""
  report_frequency = 25
  eval_sleep_secs = 10

  filter_reverse_relations = true
  use_metadata = false
}

# ACE2005 NER task parameters
ace05_best_ner = ${mtl_best}{
  mention_ratio = 0.3
  max_arg_width = 8
  add_ner_emb = 0
  ns_randp = 1
  bilinear = 0

  dropout_rate = 0.4
  learning_rate = 0.001
  coref_weight = 0
  ner_weight = 1
  relation_weight = 1.0
  main_metrics = ner

  ner_labels = ["LOC", "WEA", "GPE", "PER", "FAC", "ORG", "VEH"]
  relation_labels = ["PHYS", "PART-WHOLE", "ART", "ORG-AFF", "PER-SOC", "GEN-AFF"]
  lm_path = "../data/ace05/elmo/train.hdf5"
  train_path = "../data/ace05/json/train.json"
  lm_path_dev = "../data/ace05/elmo/dev.hdf5"
  eval_path = "../data/ace05/json/dev.json"
  output_path = ./ace_best_relation_coarse2fine_relprop_relu2_coref2_1126.dev.json
  coref_only = 0

  batch_size = 30
  coarse_to_fine = 1
  train_path_coref = "../data/conll_ner/json/train.english.json"
  lm_path_coref = "../data/conll/elmo/train.english.hdf5"
  eval_path_coref = "../data/conll_ner/json/dev.english.json"
  lm_path_dev_coref = "../data/conll/elmo/dev.english.hdf5"
  entity_ratio = 0.5
  entity_beam = 0

  gpu = 0
  coref_freq = 2
  coref_depth = 2
  rel_prop = 2
  rel_prop_emb = 1
}
